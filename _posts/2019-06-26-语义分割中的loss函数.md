---
layout:     post
title:      "loss"
subtitle:   "语义分割中的loss函数"
date:       2019-06-26
author:     "柳阳飞"
tags:
        - loss
        - pytorch keras
        - scene segmentation
---
#### Log loss

log loss 就是`keras`中的*`binary_crossentropy()`*
$$
loss = -y.\log(y^,) - (1-y)\log(1-y^,)\\
y = y_{truth}, y^, = y_{pred}
$$
对上式进行拆分可以得到
$$
\begin{equation}
loss = 
\begin{cases}
-\log(y^,)&\mbox{if y=1} \\
-\log(1-y^,)&\mbox{if y=0}
\end{cases}
\end{equation}
$$
经过拆分后可以发现，predict与target越接近，损失越小，最终的损失是两类损失的加权  和。<u>缺点就是当正样本数量远小于负样本数量时，使得训练的模型偏向于预测背景，因为这会获得更小的损失。</u>

#### Dice loss

Dice loss 常用来解决前景区域所占像素非常小的分割问题，定义为：
$$
DSC（A,B）=2\frac{|A\cap B|}{|A|+|B|}
$$
也可以表示为：
$$
DSC = \frac{2TP}{2TP+FN+FP}
$$
其中*`TP, FP, FN`*分别是真阳性、假阳性、假阴性的个数，关于`TP, TN, FP, FN`的[说明](http://www.mashangxue123.com/深度学习/2341695462.html)

二分类dice loss为：
$$
loss = 1-DSC
$$
`keras`上的实现为：

```python
def dice_coef(y_true, y_pred, smooth=1):
    intersection = K.sum(y_true * y_pred, axis=[1,2,3])
    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])
    return K.mean((2. * intersection + smooth) / (union + smooth), axis=0)
def dice_coef_loss(y_true, y_pred):
	1 - dice_coef(y_true, y_pred, smooth=1)
```

多分类dice loss的实现在大多数的[博客](https://blog.csdn.net/wangdongwei0/article/details/84576044)中都是这样写的：

```python
def dice_coef(y_true, y_pred, smooth=1):
    mean_loss = 0
    for i in range(y_pred.shape(-1)):
       intersection = K.sum(y_true[:,:,:,i] * y_pred[:,:,:,i], axis=[1,2,3])
       union = K.sum(y_true[:,:,:,i], axis=[1,2,3]) + k.sum(y_pred[:,:,:,i],axis=[1,2,3])
    mean_loss += (2. * intersection + smooth) / (union + smooth)
    return K.mean(mean_loss, axis=0)
def dice_coef_loss(y_true, y_pred):
	1 - dice_coef(y_true, y_pred, smooth=1)
```

但是参照`pytorch`上的[实现](https://blog.csdn.net/a362682954/article/details/81226427)，我觉得上面的实现应该改为：

```python
def dice_coef(y_true, y_pred, smooth=1):
    mean_loss = 0
    for i in range(y_pred.shape(-1)):
       intersection = K.sum(y_true[:,:,:,i] * y_pred[:,:,:,i], axis=[1,2,3])
       union = K.sum(y_true[:,:,:,i], axis=[1,2,3]) + k.sum(y_pred[:,:,:,i],axis=[1,2,3])
       mean_loss += (2. * intersection + smooth) / (union + smooth)
    return K.mean(mean_loss, axis=0)
def dice_coef_loss(y_true, y_pred):
	1 - dice_coef(y_true, y_pred, smooth=1)
```

#### Focal loss

focal loss公式如下：
$$
loss = -\alpha y(1-y^,)^\gamma \log y^, - (1-\alpha)(1-y){y^,}^\gamma\log(1-y^,)
$$
α 称作平衡因子，用来平衡正负样本本身的比例不均，α>0.5时可以相对增加正样本的比例；在gamma>0的情况下，focal loss更关注难分类的样本。

如 当 γ = 2，

对于正样本，即y=1时，上式只关注前项，若y'=0.9，表明这一类容易区分，那么(1-0.9)^2会很小；若y'=0.2，表明这一类难分，那么(1-0.2)^2会很大；

对于负样本，即y=0时，上式只关注后项，若y'=0.8，表明这一类难分，那么0.8^2会很大；若y'=0.1，表明这一类容易区分，那么0.1^2会很小。

#### `BCE` + DICE LOSS

```python
def bce_logdice_loss(y_true, y_pred):
    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))
```

#### IOU loss

$$
IOU = \frac{|A\cap B|}{|A|+|B|}\\
loss = -IOU
$$

```python
def IoU(y_true, y_pred, eps=1e-6):
    if np.max(y_true) == 0.0:
        return IoU(1-y_true, 1-y_pred)
    intersection = K.sum(y_true * y_pred, axis=[1,2,3])
    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection
    return -K.mean( (intersection + eps) / (union + eps), axis=0)
```

#### 参考

[1] https://blog.csdn.net/m0_37477175/article/details/83004746?utm_source=blogxgwz9

[2] https://blog.csdn.net/wangdongwei0/article/details/84576044

[3] https://blog.csdn.net/a362682954/article/details/81226427