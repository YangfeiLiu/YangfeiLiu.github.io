---
layout:     post
title:      "Pytorch教程系列（三）"
subtitle:   "翻译"
date:       2019-08-07
author:     "柳阳飞"
tags:
        - pytorch
---

以下内容是翻译自[papersapce](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)上的Pytorch教程系列，目的是为了自己学习时候记录进度督促自己，另一方面也可供需要的人汲取。

#### PyTorch 101 第四部分：深入了解Pytorch

这篇文章是针对熟悉PyTorch基础并且想要继续学习的人。在上一篇文章里我们已经介绍了如何实现一个最基本的分类网络，接下来我们将讨论如何用pytorch实现更复杂的深度学习功能。这篇文章的目标是让你明白：

1. *nn.Module，nn.Functional，nn.Parameter*之间的区别以及什么时候去使用哪一个。
2. 如何写自己的训练机制，如不同层配置不同学习率，学习率调节策略。
3. 权重初始化。

开始之前，请记住这是PyTorch系列的第三篇。

1. [PyTorch系列教程（一）](https://yangfeiliu.github.io/2019/07/18/Pytorch%E6%95%99%E7%A8%8B%E7%B3%BB%E5%88%97-%E4%B8%80/)

2. [PyTorch系列教程（二）](https://yangfeiliu.github.io/2019/07/22/Pytorch%E6%95%99%E7%A8%8B%E7%B3%BB%E5%88%97-%E4%BA%8C/)

可以在[这里](https://github.com/Paperspace/PyTorch-101-Tutorial-Series)获取所有代码。

###### nn.Module VS nn.Functional

这是非常有用的东西，尤其是阅读源码时。在pytorch中，经常使用*torch.nn.Module*类或者*torch.nn.Functional*函数来实现*layers*，我们应该用哪一个？哪一个更好呢？

正如我们在第二部分所讲，*torch.nn.Module*是pytorch的基石，使用它的方法就是先定义一个*nn.Module*类，然后调用它的*forward*方法，这是面向对象的方法。

*nn.Functional*以函数的形式提供了一些层/激活函数，可以被直接调用不需要定义类。例如，想要调整图像大小，可以直接调用*torch.nn.functional.interpolate*。

因此，我们该如何选择使用哪一个？

###### 理解状态

通常，所有的层都可以认为是一个函数，例如卷积操作就是一堆乘法和加法操作，因此可以将其认为是一个函数，但是卷积层包含了在训练过程中需要保存和更新的权重，所以从程序角度来看，这个层不仅仅是一个函数，它需要保存数据，在训练网络时需要改变。

我希望你能明白的是，卷积层保存的数据需要改变，这意味着在训练时，这个层有状态改变。对于我们而言，如果实现一个完成卷积操作的函数时，我们还需要定义一个数据结构来分别保存这一层的权重信息。

为了避免麻烦，我们可以只定义一个类来保存数据结构，卷积操作是这个类的成员方法，这将会简化我们的工作，因为我们不需要担心函数外部存在的有状态变量。因此，如果一层有权重或者定义了其他的状态，我们建议使用*nn.Module*类，例如*dropout/Batch Norm*层在训练和测试是状态不同。

另一方面，如果没有状态或者权重，可以使用*nn.functional*，例如*nn.functional.interpolate*，*nn.functional.AvgPool2d*。

尽管有上述的推理，但是大多数*nn.Module*类都有其对应的*nn.functional*，然而，在实际工作中依然应该遵循上述推理。

###### nn.Parameter

PyTorch中另一个重要的类就是*nn.Parameter*，但是在PyTorch文档中鲜有介绍。

![code1](/img/20190807/code1.png)

每一个*nn.Module*都有一个*parameters()*函数返回，也是可训练参数。我们需要隐式地定义这些参数。在*nn.Conv2d*的定义中，作者定义了相应的权重和偏置参数，然而当我们定义*net*时，我们不需要把*nn.Conv2d*的参数加到*net*的参数中，这一切通过将*nn.Conv2d*对象设置成*net*类的成员完成。

这是由*nn.Parameter*类在内部促成的，它是*Tensor*类的子类。当我们调用一个*nn.Module*对象的*parameters()*函数时，它返回*nn.Parameters*对象的成员。

事实上，*nn.Module*类的所有权重都是实现*nn.Parameter*对象，无论何时，*nn.Module*（在我们的例子中为*nn.Conv2d*）被指定为另一个*nn.Module*的成员，被指定对象的“参数”（即*nn.Conv2d*的权重）也被添加到被分配给的对象（网络对象的参数）“参数”中。这称为*nn.Module*的“参数”注入。

如果尝试将张量分配给*nn.Module*对象，除非将其定义为*nn.Parameter*对象，否则它不会显示在*parameters()*中。这样做是为了便于可能需要缓存不可微分张量的情况，例如，在RNN的情况下缓存先前的输出。

```python
class net1(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv = nn.Linear(10,5)
    self.tens = torch.ones(3,4)                       
    # This won't show up in a parameter list 
    
  def forward(self, x):
    return self.linear(x)

myNet = net1()
print(list(myNet.parameters()))

##########################################################

class net2(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv = nn.Linear(10,5) 
    self.tens = nn.Parameter(torch.ones(3,4))                       
    # This will show up in a parameter list 
    
  def forward(self, x):
    return self.linear(x)

myNet = net2()
print(list(myNet.parameters()))

##########################################################

class net3(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv = nn.Linear(10,5) 
    self.net  = net2()                      
    # Parameters of net2 will show up in list of parameters of net3
    
  def forward(self, x):
    return self.linear(x)


myNet = net3()
print(list(myNet.parameters()))
```

######   nn.ModuleList 和 nn.ParameterList()

当用PyTorch实现YOLO v3时必须使用*nn.ModuleList*，因为需要解析一个包含网络结构的文本文档来搭建网络，将所有的*nn.Module*对象存在一个列表中，然后使这个列表成为*nn.Module*的成员。简化起来就像下面这样。

![code2](/img/20190807/code2.png)

如你所见，这与我们注入单个模块不同，分配Python列表不会注入参数。为了解决这个问题，我们用*nn.ModuleList*类来包裹列表，然后作为一个成员分配到网络中。

![code3](/img/20190807/code3.png)

类似的，*Tensors*列表也可以通过包裹*nn.ParameterList*类来注入。

###### 权重初始化

权重初始化会影响训练的结果，此外，不同层可能需要不同的初始化策略，这些都可以通过*modules*和*apply*函数实现，*modules*是*nn.Module*类的成员函数，返回一个包含*nn.Module*函数所有的*nn.Module*成员的迭代器，然后使用*apply*函数调用每个*nn.Module*来初始化。

```python
import matplotlib.pyplot as plt
%matplotlib inline

class myNet(nn.Module):
 
  def __init__(self):
    super().__init__()
    self.conv = nn.Conv2d(10,10,3)
    self.bn = nn.BatchNorm2d(10)
  
  def weights_init(self):
    for module in self.modules():
      if isinstance(module, nn.Conv2d):
        nn.init.normal_(module.weight, mean = 0, std = 1)
        nn.init.constant_(module.bias, 0)

Net = myNet()
Net.weights_init()

for module in Net.modules():
  if isinstance(module, nn.Conv2d):
    weights = module.weight
    weights = weights.reshape(-1).detach().cpu().numpy()
    print(module.bias)               # Bias to zero
    plt.hist(weights)
    plt.show()
```

![fig1](/img/20190807/fig1.png)

在*torch.nn.init*模块中可以找到大量的初始化函数。

###### modules() VS children()

与*modules*非常相似的函数是*children*，它们的差别很小但是很重要。我们知道，一个*nn.Module*对象可以包含其他*nn.Module*对象做为成员。

当调用*children*时，*children()*只会返回*nn.Module*对象的数据成员列表。

另一方面，*nn.Modules*在每一个*nn.Module*对象里递归，创建每个*nn.Module*对象的列表，直到没有*nn.Module*为止。注意，*modules()*还返回已作为列表一部分调用的*nn.Module*

注意，对于从*nn.Module*类继承的所有对象/类，上述语句仍然适用。

![code4](/img/20190807/code4.png)

![fig2](/img/20190807/fig2.png)

因此，当我们初始化权重时使用*modules()*函数，因为我们无法进入*nn.Sequential*对象内部并初始化其成员的权重。

###### 打印网络信息

无论是处于用户的目的还是调试，我们可能需要打印网络信息，PyTorch提供了一个非常巧妙的方式来打印网络的信息通过使用*named_\**函数。

1. *named_parameters*，返回一个元组迭代器，包含参数的名字（如果一个卷积层被定义为*self.conv1*，参数名字就会是*conv1.weight*和*conv1.bias*），参数值通过*nn.Parameter*的*\__repr\__*函数返回。
2. *named_modules*，和上一个一样，但是返回*modules*，和*modules()*函数一样。
3. *named_children*，和上一个一样，但是返回*modules*，和*children()*函数一样。
4. *named_buffers*，返回缓冲区张量，例如BN层的平均值。

![code5](/img/20190807/code5.png)

###### 为不同层设置不同学习率

在本节中，我们将学习如何为不同的层使用不同的学习率。一般而言，我们将介绍如何针对不同的参数组设置不同的超参数，要么是不同层的学习率不同，要么是偏差和权重的学习率不同。

实现这样的想法相当简单。在我们之前的文章中，我们实现了CIFAR分类器，我们将网络的所有参数作为一个整体传递给了优化器对象。

![code6](/img/20190807/code6.png)

然而，*torch.optim*类允许我们以字典的形式为不同的参数集设置不同学习率。

```python
optimiser = torch.optim.SGD([{"params": Net.fc1.parameters(), 'lr' : 0.001, "momentum" : 0.99},
                             {"params": Net.fc2.parameters()}], lr = 0.01, momentum = 0.9)
```

在上面的列子中，*fc1*的参数使用0.01的学习率和0.99的动量，如果没有为一组参数设置超参数，它们就会使用默认值。可以使用上面介绍的*named_parameters()*函数，根据不同的层创建参数列表，或者权重和偏差。

###### 学习率调节策略

学习率是主要调节的参数，PyTorch提供学习率调节机制*torch.optim.lr_scheduler*模块，有许多不同的调节方法。

```python
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, milestones = [10,20], gamma = 0.1)
```

当我们达到epochs列表中包含的时期时，上述程序将学习率乘以$gamma$。在我们的例子中，学习率在第10和第20个时期乘以0.1。您还必须在代码中循环执行*scheduler.step*，以便更新学习速率。

通常，训练循环由两个嵌套循环组成，其中一个循环遍历epochs，嵌套的循环遍历该epochs的所有batch。确保你在epoch循环开始时调用*scheduler.step*，这样你的学习率会更新，小心不要写进batch循环中，否则你的学习率会在第10个batch更新，而不是第10个epoch。

还需要知道*scheduler.step*不能替代*optim.step*，需要在反向传播时调用*optim.step*。

###### 保存模型

您可能希望保存模型以便用于推理。在PyTorch中保存模型时，有两种选择。

1. 使用*torch.save*，这相当于使用Pickle序列化整个*nn.Module*对象，这会将整个模型保存到磁盘，你可以使用*torch.load*稍后在内存中加载这个模型。

![code7](/img/20190807/code7.png)

上面的语句将会保存整个模型的权重和结构，如果只需要保存权重而不是整个网络，你可以保存模型的*state_dict*。*state_dict*是一个字典，它将网络的*nn.Parameter*对象映射到它们的值。

如上所示，可以将现有的*state_dict*加载到*nn.Module*对象中。注意，这不保存整个模型而只保存参数。在加载*state_dict*之前，您必须使用层创建网络。如果网络结构与我们保存的*state_dict*的网络结构不完全相同，PyTorch将抛出错误。

![code8](/img/20190807/code8.png)

来自*torch.optim*的优化器对象还有一个*state_dict*对象，用于存储优化算法的超参数。它可以通过在优化器对象上调用*load_state_dict*以与上面类似的方式保存和加载。

#### 总结

这完成了我们对PyTorch的一些更高级功能的讨论。我希望你在这篇文章中看到的内容可以帮助你实现你可能想出的复杂深层学习的想法。

###### 传送门

1. [PyTorch中的学习率调节策略](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)

2. [保存和加载模型](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
3. [到底什么是*torch.nn*](https://pytorch.org/tutorials/beginner/nn_tutorial.html)

